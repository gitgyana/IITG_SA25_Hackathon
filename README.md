# ğŸŒ± [Summer Analytics First Hackathon](https://www.kaggle.com/competitions/summer-analytics-mid-hackathon)

This repository contains the solution for the first course hackathon of Summer Analytics 2025 organized by Consulting & Analytics Club (from IIT Guwahati) and GeeksforGeeks (GFG). The project focuses on classifying land cover types based on NDVI (Normalized Difference Vegetation Index) time series data using advanced feature engineering and ensemble modeling techniques.

---

## ğŸ“ Directory Structure

```
IITG_SA25_Hackathon/
â”œâ”€â”€ .git/                        # Git versioning folder
â”œâ”€â”€ kaggle/
â”‚   â””â”€â”€ input/
â”‚       â””â”€â”€ summer-analytics-mid-hackathon/
â”‚           â”œâ”€â”€ hacktrain.csv   # Training dataset
â”‚           â””â”€â”€ hacktest.csv    # Testing dataset
â”œâ”€â”€ iitg_sa_hackathon.ipynb     # Initial notebook
â”œâ”€â”€ iitg_sa_hackathon_final.ipynb  # Final notebook with highest accuracy
â”œâ”€â”€ submission.csv              # Final predictions
â””â”€â”€ README.md                   # Project documentation (this file)
```

---

## ğŸš€ Quick Start

1. Clone the repo:
   ```
   git clone https://github.com/gitgyana/IITG_SA25_Hackathon.git
   cd IITG_SA25_Hackathon
   ```

2. Make sure the datasets are in:

   ```
   kaggle/input/summer-analytics-mid-hackathon/
   ```

3. Open `iitg_sa_hackathon_final.ipynb` in Jupyter or VS Code.

4. Run all cells to reproduce the final results.

---

## ğŸ¯ Objective

Classify regions based on temporal NDVI satellite data into one of the predefined land cover classes with high accuracy. We achieve this using:

* Rich **feature engineering** from NDVI sequences
* **Outlier handling**, **missing value imputation**, and **trend detection**
* A robust **XGBoost model**, further improved with **stacking/ensemble learning**

---

## ğŸ’¡ Features Extracted

* **Basic statistics:** mean, std, min, max, median
* **Spread measures:** IQR, range, skewness, kurtosis
* **Counts:** negative NDVI values, zero NDVI values
* **Trend:** slope (NDVI trend over time)
* **Engineered combinations:** mean Ã— range, std Ã· IQR, etc.

---

## ğŸ§  Models

* `XGBClassifier` â€” base classifier
* (Optional in extension) `RandomForestClassifier` + `LogisticRegression` â€” stacked ensemble

---

## ğŸ“Š Performance

* âœ… **Validation Accuracy:** **94.56%**
* ğŸ“„ Final predictions saved in `submission.csv`

---

## ğŸ› ï¸ Requirements

Pre-installed on most Kaggle/Jupyter setups:

```bash
pandas
numpy
scikit-learn
scipy
xgboost
```

---

## ğŸ“¬ Contact

* **Name:** Gyana Priyadarshi
* **Email:** [gyana.career@gmail.com](mailto:gyana.career@gmail.com)
* **GitHub:** [github.com/gitgyana](https://github.com/gitgyana)

---
